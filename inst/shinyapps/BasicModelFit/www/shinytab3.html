<body> <div id="shinytab3" class="section level2">&#13;
<h2>What to do</h2>&#13;
<p>The model is assumed to run in units of days.</p>&#13;
<div id="task-1" class="section level3">&#13;
<h3>Task 1</h3>&#13;
<ul><li>Start with 10<sup>6</sup> uninfected cells, no infected cells, 1 virion (assumed to be in the same units of the data, TCID50/ml).</li>&#13;
<li>No uninfected cell birth and deaths, lifespan of infected cells 12 hours, unit conversion 0.</li>&#13;
<li>For the parameters that are fit, set virus production rate to 10<sup>-3</sup>, infection rate to 10<sup>-1</sup> and virus decay rate to 1. These parameters are being fit, the values we specify here are the starting conditions for the optimizer.</li>&#13;
<li>Set all “fitted” switches to YES to make sure the parameters are being fit. For each parameter, choose some lower and upper bounds. Note that if the lower bound is not lower/equal and the upper not higher/equal than the parameter, you will get an error message when you try to run the model.</li>&#13;
<li>Ignore the values for simulated data for now, set “fit to simulated data” to NO.</li>&#13;
<li>Start with a 1 fitting step/iteration and solver type 1. Run the simulation. Since you only do a single iteration, nothing is really optimized. We are just doing this so you can see the time-series produced with these starting conditions. Notice that the virus load predicted by the model and the data are already fairly close. Also record the SSR so you can compare it with the value after the fit.</li>&#13;
<li>Now fit for 100 iterations. Look at the results. The plot shows the final fit. It will be a bit better. Also, the SSR value should have gone down, indicating a better fit. Also printed below the figure are the values of the fitted parameters for this fit.</li>&#13;
<li>Repeat the same process, now fitting for 200 iterations. You should see some further improvement in SSR. That indicates the previous fit was not the ‘best’ fit. (The best fit is the one with the lowest possible SSR).</li>&#13;
</ul></div>&#13;
<div id="task-2" class="section level3">&#13;
<h3>Task 2</h3>&#13;
<ul><li>Repeat the fit, now using the solvers/optimizers “2” and “3” for fitting. Also change the number of iterations. If you computer is fast enough, keep increasing them.</li>&#13;
<li>See what the lowest SSR is you can get and record the parameter values for which you get it.</li>&#13;
</ul><p>Generally, with increasing iterations, the fits get better. An iteration is essentially a ‘try’ of the underlying code to find the best possible model. Increasing the tries usually improves the fit. In any kind of ‘real world’ setting, one should not specify a fixed number of iterations, that is just done here so things run reasonably fast. Instead, one should ask the solver to run as long as it takes until it can’t find a way to further improve the fit (don’t further reduce the SSR). The technical expression for this is that the solver has converged to the solution. This can be done with the solver used here (<code>nloptr</code> R package), but it would take too long, so we implement a “hard stop” after the specified number of iterations.</p>&#13;
</div>&#13;
<div id="task-3" class="section level3">&#13;
<h3>Task 3</h3>&#13;
<p>Ideally, with enough iterations, all solvers should reach the best fit with the lowest possible SSR. In practice, that does not always happen, often it depends on the starting conditions. Let’s explore this idea that starting values matter.</p>&#13;
<ul><li>Set everything as in task 1, but choose a starting value for virus production rate as 10<sup>-2</sup>, starting infection rate the same value, and virus decay rate of 5.</li>&#13;
<li>Run simulation for 1 fitting step. You should see a virus load curve that has the up and down seen in the real data, but it’s shifted and the SSR is higher (around 15.6) than in the previous starting condition.</li>&#13;
<li>Try the different solvers for 100 or more iterations. You’ll likely find that while the fits always improve, and they look pretty decent when graphically compared with the data, they are not quite as good (the SSR is not as low) as in the previous task, even after 1000 steps (if your computer is not fast enough, do fewer steps).</li>&#13;
</ul><p>Maybe, if we ran one of the solvers long enough, it will reach the solution we found previously. But it could also be that the solver got ‘stuck’ in what’s called a local optimum. It found a good fit, and now as it varies parameters, each new fit is worse, so the solver “thinks” it’s found the best fit, even though there are better ones further away in parameter space. Many solvers - even so-called ‘global’ solvers - can get stuck. Unfortunately, we never know if the solution is real or if the solver is stuck in a local optimum. One way to figure this out is to try different solvers and different starting conditions, and let each one run for a long time. If all return the same answer, it’s quite likely (though not guaranteed) that we found the overall best fit (lowest SSR).</p>&#13;
</div>&#13;
<div id="task-4" class="section level3">&#13;
<h3>Task 4</h3>&#13;
<ul><li>Without much comment, I asked you to set the unit conversion factor to 0 above. That essentially means that we think this process of virions being lost due to entering infected cells is negligible compared to the other removal process, clearance of virus due to other mechanisms at rate <em>d<sub>V</sub></em>. Let’s change this assumption and turn that term back on by setting <em>g=1</em>.</li>&#13;
<li>Try the above settings, running a single iteration. You’ll find a very poor fit.</li>&#13;
<li>Play around with the starting values for the fitted parameters to see if you can get an ok looking starting simulation.</li>&#13;
<li>Once you have a decent starting simulation, try the different solvers for different interations and see how good you can get. A ‘trick’ for fitting is to run for some iterations and use the reported best-fit values as new starting conditions, then do another fit with the same or a different solver.</li>&#13;
<li>The best fit I was able to find was an SSR of 4.21. You might be able to find something better. It might depend on the bounds for the parameters. If the best-fit value reported from the optimizer is the same as the lower or upper bound for that parameter, it likely means if you broaden the bounds the fits will get better. However, the parameters have biological meanings and certain values do not make sense. For instance a lower bound for the virus decay rate of 0.001/day would mean an average virus lifespan of 1000 days or around 3 years, which is not reasonable for flu in vivo.</li>&#13;
</ul><p>While that unit conversion factor shows up in most apps, it is arguably not that important if we explore our model without trying to fit it to data. But here, for fitting purposes, this is important. The experimental units are TCID50/mL, so in our model, virus load needs to have the same units. Then, to make all units work, <em>g</em> needs to have those units, i.e. convert from infectious virions at the site of infection to experimental units. Unfortunately, how one relates to the other is not quite clear. See e.g. <span class="citation">(Handel, Longini, and Antia 2007)</span> for a discussion of that. If you plan to fit models to data you collected, you need to pay attention to units and make sure what you simulate and the data you have are in agreement.</p>&#13;
</div>&#13;
<div id="task-5" class="section level3">&#13;
<h3>Task 5</h3>&#13;
<p>One major consideration when fitting these kind of mechanistic models to data is the balance between data availability and model complexity. The more and “richer” data one has available the more parameters one can estimate and therefore the more detailed a model can be. If one tries to ‘ask too much’ from the data, it leads to the problem of over-fitting - trying to estimate more parameters than can be robustly estimated for a given dataset. One way to safeguard against overfitting is by probing if the model can in principle recover estimates in a scenario where parameter values are known. To do so, we can use our model with specific parameter values and simulate data. We can then fit the model to this simulated data. If everything works, we expect that - ideally independent of the starting values for our solver - we end up with estimated best-fit parameter values that agree with the ones we used to simulate the artificial data. We’ll try this now with the app.</p>&#13;
<ul><li>Set everything as in task 1. Now set the parameter values <em>psim</em>, <em>bsim</em> and <em>dVsim</em> to the same values as their starting values.</li>&#13;
<li>Set ‘fit to simulated data’ to YES. Run for 1 fitting step. You should now see that the data has changed. Instead of the real data, we now use simulated data. Since the parameter values for the simulated data and the starting values for the fitting routine are the same, the time-series is on top of the data and the SSR is (up to rounding errors) 0.</li>&#13;
</ul></div>&#13;
<div id="task-6" class="section level3">&#13;
<h3>Task 6</h3>&#13;
<p>Let’s see if the fitting routine can recover parameters from a simulation if we start with different initial guesses.</p>&#13;
<ul><li>Set value for simulated data parameters to 10<sup>-2</sup> for <em>psim</em> and <em>bsim</em> and 2 for <em>dVsim</em>.</li>&#13;
<li>Run for 1 fitting step. You’ll see the data change compared to before. The SSR should increase to 3.26.</li>&#13;
<li>If you now run the fitting for many steps, what do you expect the final fit values for the parameters and the SSR to be?</li>&#13;
<li>Test your expectation by running for 100+ fitting steps with the different solvers.</li>&#13;
</ul></div>&#13;
<div id="task-7" class="section level3">&#13;
<h3>Task 7</h3>&#13;
<p>If you ran things long enough in the previous task you should have obtained best fit values that were the same as the ones you used to produce the simulated data, and the SSR should have been close to 0. That indicates that you can estimate these parameters with that kind of data. Once you’ve done this test, you can be somewhat confident that fitting your model to the real data will allow you to get robust parameter estimates.</p>&#13;
<ul><li>Play around with different values for the simulated parameters and different values for the starting conditions and see if you find scenarios where you might not be able to get the solver to converge to a solution that agrees with the one you started with.</li>&#13;
</ul></div>&#13;
<div id="task-8" class="section level3">&#13;
<h3>Task 8</h3>&#13;
<ul><li>To make things a bit more realistic and harder, one can also add noise on top of the simulated data. Try that by playing with the ‘noise added’ parameter and see how well you can recover the parameter values for the simulation.</li>&#13;
</ul><p>Note that since you now change your data after you simulated it, you don’t expect the parameter values for the simulation and those you obtain from your best fit to be the same. However, if the noise is not too large, you expect them to be similar.</p>&#13;
</div>&#13;
<div id="task-9" class="section level3">&#13;
<h3>Task 9</h3>&#13;
<ul><li>Keep exploring. Fitting these kind of models can be tricky at times, and you might find strange behavior in this app that you don’t expect. Try to get to the bottom of what might be going on. This is an open-ended exploration, so I can’t really give you a “hint”. Just try different things, try to understand as much as possible of what you observe.</li>&#13;
</ul></div>&#13;
</div> </body>
